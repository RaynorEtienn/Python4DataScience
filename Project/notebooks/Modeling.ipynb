{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a082c",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89299dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.features\n",
    "import importlib\n",
    "importlib.reload(src.features)\n",
    "from src.features import extract_user_attributes, generate_training_data\n",
    "\n",
    "print(\"Regenerating Training Features from Raw Data (Snapshot Approach)...\")\n",
    "train_raw_path = '../data/train.parquet'\n",
    "train_df_raw = pd.read_parquet(train_raw_path)\n",
    "train_df_raw = extract_user_attributes(train_df_raw)\n",
    "\n",
    "# --- SMART DATA GENERATION ---\n",
    "# We use the strategy that gave us the All-Time High (0.648).\n",
    "# - Churners: Snapshots at 1, 3, 7 days before churn.\n",
    "# - Non-Churners: Random active snapshots + \"Dormancy\" snapshots (after last event).\n",
    "# This aligns the training distribution with the test set (where many users are dormant).\n",
    "\n",
    "df = generate_training_data(train_df_raw)\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Churn Rate: {df['target'].mean():.2%}\")\n",
    "\n",
    "print(\"\\nAvg Gap for Non-Churners (Target 0) - Should include large gaps (Dormancy):\")\n",
    "print(df[df['target']==0]['days_since_last_session'].describe())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d77bd6",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Splitting\n",
    "\n",
    "We will use a **Stratified Split** to maintain the churn ratio in both training and test sets.\n",
    "We will also define a `ColumnTransformer` to handle:\n",
    "- **Numerical Features**: Standard Scaling.\n",
    "- **Categorical Features**: One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cols_to_drop = [\"target\", \"userId\", \"cutoff_ts\"]\n",
    "X = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "y = df['target']\n",
    "\n",
    "categorical_cols = ['gender', 'level', 'platform']\n",
    "numerical_cols = [c for c in X.columns if c not in categorical_cols]  # type: ignore\n",
    "\n",
    "print(f\"Categorical Columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical Columns ({len(numerical_cols)}): {numerical_cols[:5]} ...\")\n",
    "\n",
    "# GroupShuffleSplit to ensure no user appears in both train and test (prevent leakage)\n",
    "splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=RANDOM_SEED)\n",
    "train_idx, test_idx = next(splitter.split(X, y, groups=df['userId']))\n",
    "\n",
    "X_train = X.iloc[train_idx]\n",
    "X_test = X.iloc[test_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "\n",
    "print(f\"\\nTraining Shape: {X_train.shape}\")\n",
    "print(f\"Test Shape: {X_test.shape}\")\n",
    "print(f\"Train Churn Rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test Churn Rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FEATURE SELECTION (DISABLED) ---\n",
    "# The All-Time High model used ALL features.\n",
    "# Dropping features might be removing subtle signals needed for the Stacking model.\n",
    "\n",
    "print(\"--- Feature Selection Disabled ---\")\n",
    "print(f\"Using all {X_train.shape[1]} features.\")\n",
    "\n",
    "# We keep the code here for reference but do not execute the drop.\n",
    "\"\"\"\n",
    "from lightgbm import LGBMClassifier\n",
    "# ... (Feature selection code omitted) ...\n",
    "X_train = X_train.drop(columns=features_to_drop)\n",
    "X_test = X_test.drop(columns=features_to_drop)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, numerical_cols),\n",
    "        ('cat', cat_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(f\"Processed Feature Matrix Shape: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23277518",
   "metadata": {},
   "source": [
    "## 3. Baseline Model Evaluation\n",
    "\n",
    "We will evaluate the following industry-standard models:\n",
    "1.  **Logistic Regression**: Simple baseline for interpretability.\n",
    "2.  **Random Forest**: Robust bagging ensemble.\n",
    "3.  **XGBoost**: Gradient boosting (often SOTA for tabular data).\n",
    "4.  **LightGBM**: Faster and often more accurate gradient boosting.\n",
    "5.  **CatBoost**: Excellent for categorical features (though we OHE them here).\n",
    "\n",
    "**Metrics**:\n",
    "- **F1-Score**: Harmonic mean of precision and recall (crucial for imbalanced churn).\n",
    "- **ROC-AUC**: Ability to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f194f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# We REMOVE explicit class weights.\n",
    "# The \"Smart Sampling\" (Dormancy Snapshots) handles the distribution better.\n",
    "# Stacking with Logistic Regression will handle the final calibration.\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=RANDOM_SEED),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=RANDOM_SEED, verbose=-1),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=RANDOM_SEED)\n",
    "}\n",
    "\n",
    "def evaluate_models(models, X, y, preprocessor):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', model)])\n",
    "        \n",
    "        # Cross-validation (5-fold)\n",
    "        cv_results = cross_validate(clf, X, y, cv=5, scoring=['f1', 'roc_auc', 'accuracy'])\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"F1 Score (Mean)\": cv_results['test_f1'].mean(),\n",
    "            \"F1 Score (Std)\": cv_results['test_f1'].std(),\n",
    "            \"ROC-AUC (Mean)\": cv_results['test_roc_auc'].mean(),\n",
    "            \"ROC-AUC (Std)\": cv_results['test_roc_auc'].std(),\n",
    "            \"Accuracy (Mean)\": cv_results['test_accuracy'].mean(),\n",
    "            \"Accuracy (Std)\": cv_results['test_accuracy'].std(),\n",
    "        })\n",
    "        print(f\"Evaluated {name}...\")\n",
    "        \n",
    "    return pd.DataFrame(results).sort_values(by=\"F1 Score (Mean)\", ascending=False)\n",
    "\n",
    "results_df = evaluate_models(models, X_train, y_train, preprocessor)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717d0d8",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning\n",
    "\n",
    "We will now optimize the hyperparameters for our top two performing models: **XGBoost** and **CatBoost**.\n",
    "We use `RandomizedSearchCV` which is more efficient than Grid Search as it samples a fixed number of parameter settings from specified distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce21f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "# Reduce Optuna verbosity to show only progress bars\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "N_TRIALS = 30  # Reverting to 30 (ATH Strategy)\n",
    "OPTIMIZATION_METRIC = 'f1'  # Reverting to F1 (ATH Strategy)\n",
    "SKIP_OPTUNA = False  # Faster iteration by skipping Optuna tuning\n",
    "\n",
    "\n",
    "def tune_model(model, params, X, y, preprocessor, n_iter=20):\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model)])\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline, \n",
    "        param_distributions=params, \n",
    "        n_iter=n_iter, \n",
    "        scoring='f1', \n",
    "        cv=3, \n",
    "        verbose=1, \n",
    "        random_state=RANDOM_SEED, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    search.fit(X, y)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTUNA OPTIMIZATION (XGBoost) ---\n",
    "if not SKIP_OPTUNA:\n",
    "    print(\"ðŸš€ Starting Optuna Optimization for XGBoost...\")\n",
    "    def objective_xgb(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': RANDOM_SEED,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "        \n",
    "        # We use cross_val_score with the specified metric\n",
    "        # CRITICAL FIX: n_jobs=1 to avoid nested parallelism deadlock (Model uses n_jobs=-1)\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=OPTIMIZATION_METRIC, n_jobs=1)\n",
    "        return scores.mean()\n",
    "\n",
    "    study_xgb = optuna.create_study(direction='maximize')\n",
    "    study_xgb.optimize(objective_xgb, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best XGBoost {OPTIMIZATION_METRIC}: {study_xgb.best_value:.4f}\")\n",
    "    best_xgb_params = study_xgb.best_params\n",
    "    best_xgb_params['eval_metric'] = 'logloss'\n",
    "    best_xgb_params['random_state'] = RANDOM_SEED\n",
    "    best_xgb_params['n_jobs'] = -1\n",
    "    best_xgb = XGBClassifier(**best_xgb_params)\n",
    "else:\n",
    "    print(\"â© Skipping Optuna for XGBoost. Using Default Params.\")\n",
    "    best_xgb = XGBClassifier(eval_metric='logloss', random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a109cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTUNA OPTIMIZATION (LightGBM) ---\n",
    "if not SKIP_OPTUNA:\n",
    "    print(\"ðŸš€ Starting Optuna Optimization for LightGBM...\")\n",
    "    def objective_lgbm(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'random_state': RANDOM_SEED,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        model = LGBMClassifier(**params)\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "        \n",
    "        # CRITICAL FIX: n_jobs=1 to avoid nested parallelism deadlock (Model uses n_jobs=-1)\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=OPTIMIZATION_METRIC, n_jobs=1)\n",
    "        return scores.mean()\n",
    "\n",
    "    study_lgbm = optuna.create_study(direction='maximize')\n",
    "    study_lgbm.optimize(objective_lgbm, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best LightGBM {OPTIMIZATION_METRIC}: {study_lgbm.best_value:.4f}\")\n",
    "    best_lgbm_params = study_lgbm.best_params\n",
    "    best_lgbm_params['random_state'] = RANDOM_SEED\n",
    "    best_lgbm_params['n_jobs'] = -1\n",
    "    best_lgbm_params['verbose'] = -1\n",
    "    best_lgbm = LGBMClassifier(**best_lgbm_params)\n",
    "else:\n",
    "    print(\"â© Skipping Optuna for LightGBM. Using Default Params.\")\n",
    "    best_lgbm = LGBMClassifier(random_state=RANDOM_SEED, verbose=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb598e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTUNA OPTIMIZATION (CatBoost) ---\n",
    "if not SKIP_OPTUNA:\n",
    "    print(\"ðŸš€ Starting Optuna Optimization for CatBoost...\")\n",
    "    def objective_cat(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "            'verbose': 0,\n",
    "            'random_state': RANDOM_SEED,\n",
    "            'thread_count': -1\n",
    "        }\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "        \n",
    "        # CRITICAL FIX: n_jobs=1 to avoid nested parallelism deadlock (Model uses thread_count=-1)\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=OPTIMIZATION_METRIC, n_jobs=1)\n",
    "        return scores.mean()\n",
    "\n",
    "    study_cat = optuna.create_study(direction='maximize')\n",
    "    study_cat.optimize(objective_cat, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best CatBoost {OPTIMIZATION_METRIC}: {study_cat.best_value:.4f}\")\n",
    "    best_cat_params = study_cat.best_params\n",
    "    best_cat_params['verbose'] = 0\n",
    "    best_cat_params['random_state'] = RANDOM_SEED\n",
    "    best_cat_params['thread_count'] = -1\n",
    "    best_cat = CatBoostClassifier(**best_cat_params)\n",
    "else:\n",
    "    print(\"â© Skipping Optuna for CatBoost. Using Default Params.\")\n",
    "    best_cat = CatBoostClassifier(verbose=0, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045ccc0",
   "metadata": {},
   "source": [
    "## 5. Ensemble Modeling (Stacking)\n",
    "\n",
    "We will now combine our tuned **XGBoost** and **CatBoost** models using a `StackingClassifier`.\n",
    "This technique uses a meta-model (Logistic Regression) to learn the best combination of the base models' predictions.\n",
    "We use the optimal hyperparameters found in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83745cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# --- HYPERPARAMETER INTEGRATION ---\n",
    "try:\n",
    "    xgb_final = best_xgb\n",
    "    print(\"âœ… Using Optimized XGBoost Params\")\n",
    "except NameError:\n",
    "    print(\"âš ï¸ Optuna cells not run: Using Default XGBoost Params\")\n",
    "    xgb_final = XGBClassifier(eval_metric='logloss', random_state=RANDOM_SEED)\n",
    "\n",
    "try:\n",
    "    lgbm_final = best_lgbm\n",
    "    print(\"âœ… Using Optimized LightGBM Params\")\n",
    "except NameError:\n",
    "    print(\"âš ï¸ Optuna cells not run: Using Default LightGBM Params\")\n",
    "    lgbm_final = LGBMClassifier(random_state=RANDOM_SEED, verbose=-1)\n",
    "\n",
    "try:\n",
    "    cat_final = best_cat\n",
    "    print(\"âœ… Using Optimized CatBoost Params\")\n",
    "except NameError:\n",
    "    print(\"âš ï¸ Optuna cells not run: Using Default CatBoost Params\")\n",
    "    cat_final = CatBoostClassifier(verbose=0, random_state=RANDOM_SEED)\n",
    "\n",
    "# Removed KNN from the stack\n",
    "estimators: list[tuple[str, BaseEstimator]] = [\n",
    "    ('xgb', Pipeline(steps=[('preprocessor', preprocessor), ('classifier', xgb_final)])),\n",
    "    ('lgbm', Pipeline(steps=[('preprocessor', preprocessor), ('classifier', lgbm_final)])),\n",
    "    ('cat', Pipeline(steps=[('preprocessor', preprocessor), ('classifier', cat_final)]))\n",
    "]\n",
    "\n",
    "# --- LEADERBOARD CHECK: INDIVIDUAL MODEL EVALUATION ---\n",
    "# Before stacking, we check if any single model is better than the ensemble.\n",
    "# This helps us detect if the ensemble is just adding noise.\n",
    "\n",
    "print(\"\\n--- INDIVIDUAL MODEL EVALUATION (5-Fold CV) ---\")\n",
    "for name, pipe in estimators:\n",
    "    print(f\"\\nEvaluating {name.upper()}...\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        cv_res = cross_validate(pipe, X_train, y_train, cv=5, scoring=['f1', 'roc_auc'])\n",
    "    print(f\"{name.upper()} F1: {cv_res['test_f1'].mean():.4f} (+/- {cv_res['test_f1'].std():.4f})\")\n",
    "    print(f\"{name.upper()} AUC: {cv_res['test_roc_auc'].mean():.4f}\")\n",
    "\n",
    "# EXPERIMENT 11: SOFT VOTING\n",
    "# Stacking (Exp 7-10) proved unstable. We switch to Soft Voting.\n",
    "# This averages the probabilities from all models, reducing variance.\n",
    "# It is less prone to overfitting than Stacking's meta-learner.\n",
    "\n",
    "stacking_clf = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating Soft Voting Classifier...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    cv_results_stack = cross_validate(stacking_clf, X_train, y_train, cv=5, scoring=['f1', 'roc_auc', 'accuracy'])\n",
    "\n",
    "print(f\"Voting F1 Score: {cv_results_stack['test_f1'].mean():.4f} (+/- {cv_results_stack['test_f1'].std():.4f})\")\n",
    "print(f\"Voting ROC-AUC: {cv_results_stack['test_roc_auc'].mean():.4f}\")\n",
    "print(f\"Voting Accuracy: {cv_results_stack['test_accuracy'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad27c9",
   "metadata": {},
   "source": [
    "## 6. Threshold Optimization\n",
    "\n",
    "Standard models use a default threshold of 0.5 to classify a user as \"Churn\" or \"Not Churn\".\n",
    "However, since our dataset is imbalanced and we care deeply about F1-Score, this default is rarely optimal.\n",
    "\n",
    "We will:\n",
    "1.  Generate probability predictions using Cross-Validation (to avoid overfitting).\n",
    "2.  Iterate through all possible thresholds (0.01 to 0.99).\n",
    "3.  Find the threshold that maximizes the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Generating cross-validated predictions...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    y_scores = cross_val_predict(stacking_clf, X_train, y_train, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_scores >= thresh).astype(int)\n",
    "    f1_scores.append(f1_score(y_train, y_pred_thresh))\n",
    "\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "max_f1_score = f1_scores[optimal_idx]\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"Max Cross-Validated F1 Score: {max_f1_score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, label='F1 Score')\n",
    "plt.axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal Threshold ({optimal_threshold:.2f})')\n",
    "plt.title('F1 Score vs Decision Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d22b2",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation on Test Set\n",
    "\n",
    "Now we perform the final \"exam\". We will:\n",
    "1.  Retrain the Stacking Model on the **full training set**.\n",
    "2.  Predict probabilities on the **held-out test set**.\n",
    "3.  Apply our **Optimal Threshold** to generate final class predictions.\n",
    "4.  Report the final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1587640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"Retraining Stacking Model on full training set...\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Final Test Set Performance ---\")\n",
    "\n",
    "# Handle Hard Voting (no predict_proba)\n",
    "if hasattr(stacking_clf, 'predict_proba'):\n",
    "    y_test_probs = stacking_clf.predict_proba(X_test)[:, 1]  # type: ignore\n",
    "    y_test_pred = (y_test_probs >= optimal_threshold).astype(int)\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_test_probs):.4f}\")\n",
    "else:\n",
    "    print(\"Hard Voting detected: Skipping ROC-AUC and Thresholding.\")\n",
    "    y_test_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b05dcd",
   "metadata": {},
   "source": [
    "## 8. Generate Submission File\n",
    "\n",
    "We will now generate predictions for the **Test Dataset** (`data/test.parquet`).\n",
    "Since this is likely raw data, we need to apply the same **Feature Engineering** steps as we did for the training data.\n",
    "\n",
    "**Steps**:\n",
    "1.  Load `test.parquet`.\n",
    "2.  Apply `extract_user_attributes` and `aggregate_user_features`.\n",
    "3.  Generate probabilities using the trained `stacking_clf`.\n",
    "4.  Apply the `optimal_threshold`.\n",
    "5.  Save to `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.features\n",
    "importlib.reload(src.features)\n",
    "from src.features import extract_user_attributes, aggregate_user_features\n",
    "\n",
    "# 1. Load Raw Test Data\n",
    "test_data_path = '../data/test.parquet'\n",
    "print(f\"Loading test data from {test_data_path}...\")\n",
    "test_df_raw = pd.read_parquet(test_data_path)\n",
    "\n",
    "# 2. Apply Feature Engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "test_df_raw = extract_user_attributes(test_df_raw)\n",
    "\n",
    "# --- CRITICAL FIX FOR TEST SET ---\n",
    "# The test set is a snapshot of the \"current\" state.\n",
    "# We must calculate features relative to the END of the test period (Global Max Timestamp),\n",
    "# NOT relative to each user's last event.\n",
    "# If we use user's last event (default), 'days_since_last_session' would be 0 for everyone,\n",
    "# destroying the most important churn signal (inactivity).\n",
    "\n",
    "global_max_ts = test_df_raw['ts'].max()\n",
    "print(f\"Global Test Cutoff Timestamp: {global_max_ts}\")\n",
    "\n",
    "# Create a snapshot dataframe for test users\n",
    "test_snapshot_df = pd.DataFrame({\n",
    "    'userId': test_df_raw['userId'].unique(),\n",
    "    'cutoff_ts': global_max_ts\n",
    "})\n",
    "\n",
    "# Generate features using this common cutoff\n",
    "test_features = aggregate_user_features(test_df_raw, snapshot_df=test_snapshot_df)\n",
    "\n",
    "# The result has a MultiIndex (userId, cutoff_ts), we drop the cutoff_ts level\n",
    "test_features = test_features.reset_index(level='cutoff_ts', drop=True)\n",
    "\n",
    "# --- FEATURE ENGINEERING BOOST (Must match Training) ---\n",
    "if 'sessions_last_7d' in test_features.columns and 'sessions_last_28d' in test_features.columns:\n",
    "    test_features['session_velocity'] = test_features['sessions_last_7d'] / (test_features['sessions_last_28d'] / 4 + 0.01)\n",
    "\n",
    "if 'days_since_last_session' in test_features.columns and 'total_sessions' in test_features.columns:\n",
    "    test_features['recency_frequency_ratio'] = test_features['days_since_last_session'] / (test_features['total_sessions'] + 1)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Drop 'target' if it exists\n",
    "if 'target' in test_features.columns:\n",
    "    test_features = test_features.drop(columns=['target'])\n",
    "\n",
    "# Ensure columns match training data\n",
    "missing_cols = set(X_train.columns) - set(test_features.columns)\n",
    "for c in missing_cols:\n",
    "    test_features[c] = 0\n",
    "\n",
    "# Reorder columns to match X_train\n",
    "test_features = test_features[X_train.columns]\n",
    "\n",
    "print(f\"Test Features Shape: {test_features.shape}\")\n",
    "print(f\"Avg Days Since Last Session: {test_features['days_since_last_session'].mean():.2f} days\")\n",
    "\n",
    "# 3. Generate Predictions\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Helper function to save submission\n",
    "def save_submission(model, name, features, threshold=0.5):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probs = model.predict_proba(features)[:, 1]\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "    else:\n",
    "        preds = model.predict(features)\n",
    "    \n",
    "    sub_df = pd.DataFrame({'id': features.index, 'target': preds})\n",
    "    path = f'../data/submission_{name}.csv'\n",
    "    sub_df.to_csv(path, index=False)\n",
    "    print(f\"Saved {name} submission to {path}\")\n",
    "    return path\n",
    "\n",
    "# A. Generate Individual Submissions (using optimal threshold from Voting, or 0.5?)\n",
    "# Ideally we should optimize threshold for each, but for now we use the Voting threshold\n",
    "# as a reasonable proxy, or just 0.5 if we want raw comparison.\n",
    "# Let's use the optimal_threshold found for the ensemble to be consistent.\n",
    "\n",
    "print(f\"Using Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# B. Generate Voting Submission\n",
    "print(\"Generating Voting Ensemble predictions...\")\n",
    "# stacking_clf is already fitted on full data? No, we need to fit it!\n",
    "# Wait, did we fit it?\n",
    "# In Section 7 step 1 (not shown in this snippet but usually there), we usually fit the final model.\n",
    "# Let's ensure it is fitted.\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "submission_path = save_submission(stacking_clf, \"voting\", test_features, optimal_threshold)\n",
    "\n",
    "# Overwrite the main submission file for compatibility\n",
    "import shutil\n",
    "shutil.copy(submission_path, '../data/submission.csv')\n",
    "print(\"Copied voting submission to ../data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138604dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "message = f\"Raynor's attempt at {current_time}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e257063",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ../../.venv/bin/kaggle competitions submit -c churn-prediction-25-26 -f ../data/submission.csv -m \"{message} (Voting)\"\n",
    "# Uncomment to submit others:\n",
    "# ! ../../.venv/bin/kaggle competitions submit -c churn-prediction-25-26 -f ../data/submission_xgb.csv -m \"{message} (XGB)\"\n",
    "# ! ../../.venv/bin/kaggle competitions submit -c churn-prediction-25-26 -f ../data/submission_lgbm.csv -m \"{message} (LGBM)\"\n",
    "# ! ../../.venv/bin/kaggle competitions submit -c churn-prediction-25-26 -f ../data/submission_cat.csv -m \"{message} (Cat)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e638b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAVE MODEL ARTIFACTS ---\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving artifacts to {models_dir}...\")\n",
    "\n",
    "# 1. Save the Stacking Model\n",
    "model_path = os.path.join(models_dir, \"stacking_model.joblib\")\n",
    "joblib.dump(stacking_clf, model_path)\n",
    "print(f\"- Model saved to {model_path}\")\n",
    "\n",
    "# 2. Save the Preprocessor\n",
    "preprocessor_path = os.path.join(models_dir, \"preprocessor.joblib\")\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"- Preprocessor saved to {preprocessor_path}\")\n",
    "\n",
    "# 3. Save the Optimal Threshold\n",
    "threshold_path = os.path.join(models_dir, \"optimal_threshold.joblib\")\n",
    "joblib.dump(optimal_threshold, threshold_path)\n",
    "print(f\"- Optimal Threshold ({optimal_threshold:.4f}) saved to {threshold_path}\")\n",
    "\n",
    "# 4. Save Feature Names (Crucial for alignment)\n",
    "features_path = os.path.join(models_dir, \"feature_names.joblib\")\n",
    "joblib.dump(X_train.columns, features_path)\n",
    "print(f\"- Feature names saved to {features_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "print(\"--- AUTOMATED REPORT FOR AI AGENT ---\")\n",
    "print(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Gather info\n",
    "report = {\n",
    "    \"optimization_enabled\": not SKIP_OPTUNA,\n",
    "    \"data_shape\": X_train.shape,\n",
    "    \"features\": list(X_train.columns),\n",
    "    \"model_performance_cv\": {\n",
    "        \"f1_mean\": cv_results_stack[\"test_f1\"].mean(),\n",
    "        \"f1_std\": cv_results_stack[\"test_f1\"].std(),\n",
    "        \"roc_auc_mean\": cv_results_stack[\"test_roc_auc\"].mean(),\n",
    "        \"accuracy_mean\": cv_results_stack[\"test_accuracy\"].mean(),\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"max_f1_at_threshold\": max_f1_score,\n",
    "    },\n",
    "    \"best_params\": {\n",
    "        \"trials\": N_TRIALS,\n",
    "        \"xgb\": best_xgb_params if \"best_xgb_params\" in locals() else \"Default\",\n",
    "        \"f1_xgb_best_perf\": (\n",
    "            study_xgb.best_value if \"study_xgb\" in locals() else \"Default\"\n",
    "        ),\n",
    "        \"lgbm\": best_lgbm_params if \"best_lgbm_params\" in locals() else \"Default\",\n",
    "        \"f1_lgbm_best_perf\": (\n",
    "            study_lgbm.best_value if \"study_lgbm\" in locals() else \"Default\"\n",
    "        ),\n",
    "        \"cat\": best_cat_params if \"best_cat_params\" in locals() else \"Default\",\n",
    "        \"f1_cat_best_perf\": (\n",
    "            study_cat.best_value if \"study_cat\" in locals() else \"Default\"\n",
    "        )\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(report, indent=2, default=str))\n",
    "print(\"-------------------------------------\")\n",
    "print('\"Kaggle Score\": ???')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
