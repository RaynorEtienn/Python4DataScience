{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e896d74-382a-4717-b507-73d09f4b433d",
   "metadata": {},
   "source": [
    "## Lab 3: Pandas, sklearn and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84417f",
   "metadata": {},
   "source": [
    "## - Training a model and tuning its hyperparameter -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00dd7ce-bbe2-431f-86e1-9853f12b18aa",
   "metadata": {},
   "source": [
    "1) What kind of problem is it? Regression of classification? Supervised or unsupervised?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e2746-18fd-4849-bb68-16dd071b2c8d",
   "metadata": {},
   "source": [
    "We are here asked to predict log_PAX, which is the target variable and is the number of passengers on a given flight. We are thus facing a regression problem, and it is supervised since we are using labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4090af-4c46-473d-a193-26651d4c4baf",
   "metadata": {},
   "source": [
    "2) Load the training data from Moodle (train.csv.bz2; bz2 is a compression format, pandas can\n",
    "decompress it itself). The target variable is called log_PAX. Do a quick inspection of the dataset.\n",
    "What are the types of the columns ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a483d15e-f303-4140-a4fe-38a150f27442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1f6a5-d2cb-4a4e-ae6f-29d73cecc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf157a-eaf9-4bd2-8bba-a217fd89f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41d8f6-eacf-4574-8377-5b596379b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c005b4-1885-4802-a442-3a47bcd38864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2eced5-7e75-4a3c-9592-787818550130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830d748-75bd-4dcc-bcf3-bea1370a0555",
   "metadata": {},
   "source": [
    "3) Convert dates to proper dates. Create new integers columns containing respectively the day (day\n",
    "of the month: from 1 to 31), the weekday (day of the week: from 1 to 7), the week, the month, the\n",
    "year, a binary variable indicating if this is a bank holiday (in the US calendar), a binary variable\n",
    "indicating if this is the weekend or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f5284-f210-464c-a462-7d226bc52ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DateOfDeparture']= pd.to_datetime(df['DateOfDeparture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26856dd-efd4-4165-9d62-5ef635565d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5831ea-2db3-4e44-b2b5-bd8f362d6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day']= df['DateOfDeparture'].dt.day\n",
    "df['weekday']= df['DateOfDeparture'].dt.weekday +1\n",
    "df['week']= df['DateOfDeparture'].dt.isocalendar().week.astype(int)\n",
    "df['month']= df['DateOfDeparture'].dt.month\n",
    "df['year']= df['DateOfDeparture'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723e464-071f-4508-b73b-cc6d92b79db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_weekend']= df['weekday'].isin([6,7]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d22c48-7e0e-4173-935e-0b967693fa1d",
   "metadata": {},
   "source": [
    "We don't know when the holidays are in the US, so we fix it to 0 for now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddfe9d-eb3d-4cc6-85ff-c4cc3f59a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_holiday'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbda14-2127-46b4-ba20-21d472c5a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c574f88-dc97-46b0-aa0e-ff2de31998cb",
   "metadata": {},
   "source": [
    "4) First, select numerical features in an automated fashion (not by hand). You can for example use a\n",
    "list comprehension, or df.select_dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534c156-ed03-453f-a468-f90e759722c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0b217-23f1-48cb-81d7-a6df5eb77842",
   "metadata": {},
   "source": [
    "This list comprehension included in pandas allows us to detect numeric columns automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdffaa4-7713-4e77-b01e-886723150990",
   "metadata": {},
   "source": [
    "5) We will use the Root Mean Squared Error (RMSE) as a figure of merit (performance measure) for\n",
    "this prediction task. Explain how it is defined and why it is relevant here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34426e-fcc7-4f86-b47a-959a54b06e50",
   "metadata": {},
   "source": [
    "The RMSE helps us measure the average errors of the predictions.\n",
    "We can compute it using sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d34dc-db4d-48cb-8525-1a6138a40f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7e3d2-3034-48e8-8df5-0d10bfb8342e",
   "metadata": {},
   "source": [
    "6) Do a train-test split of the data (a single one, so far. You’ll do K-fold cross validation later) and\n",
    "tune the max_depth parameter of a DecisionTreeRegressor. Explain briefly how this estimator\n",
    "does its prediction. Plot the RMSE on train and test sets as a function of this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae905dee-95fb-407d-a89c-800935bf320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df[num_cols].drop(columns=['log_PAX'])\n",
    "y= df['log_PAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4da3f-aa92-41a5-855f-4921df4884f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38528bc9-3312-42d8-a911-be1fef1493d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2b9fb-a070-4051-84df-e99bccd907c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e712d57-aa3a-487e-8a9c-09c2be4489ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535bbfd-1c03-4413-922b-14b8debf0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2a043-8e97-4242-91b3-887f9cf39c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ce3bf-2955-407e-810e-c4906fa187e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, test_rmse = [], []\n",
    "depths = range(1, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb971ad-3ca6-4a8e-b506-5a6bd25e88c6",
   "metadata": {},
   "source": [
    "We try depths from 1 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0764b-c718-43a7-b8ad-8a4093d56bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in depths:\n",
    "    tree = DecisionTreeRegressor(max_depth=d, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = tree.predict(X_train)\n",
    "    y_test_pred = tree.predict(X_test)\n",
    "\n",
    "    rmse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    train_rmse.append(rmse_train)\n",
    "    test_rmse.append(rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c6e39-7934-4c0f-8035-e96457e5259e",
   "metadata": {},
   "source": [
    "We can plot the RMSE curve: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e344c-afb5-49c0-b339-81f1d1154a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(depths, train_rmse, marker=\"o\", label=\"Train RMSE\")\n",
    "plt.plot(depths, test_rmse, marker=\"s\", label=\"Test RMSE\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955ed1d-8643-4ece-a669-456ac002217c",
   "metadata": {},
   "source": [
    "We see that for small depths, bot train and test RMSE are high (underfitting). \n",
    "Whereas for large depts the train RMSE gets smaller while the test RMSE increases.\n",
    "The best depth will be at the minimum of the test RMSE, which is at approximately 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e847ea-9b85-47e0-ae5f-870ab54131b5",
   "metadata": {},
   "source": [
    "The decision tree regressor predicts by splitting the data into regions with similar target values and returning the average target(mean) of the training samples in the corresponding leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc075da8-4d79-47f5-aefc-ce186269a312",
   "metadata": {},
   "source": [
    "7) Test the impact of using or not a StandardScaler on the features, for this estimator with the found\n",
    "value of max_depth (use a Pipeline). Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc606855-ed4d-4441-95df-4af1555637f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c974d-a8e4-4414-bcee-f0445cf620d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d6ed9-a851-4ba5-a4a6-330b33de3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_no_scaler = Pipeline([('model', DecisionTreeRegressor(max_depth=best_depth, random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58712914-a786-4d0f-8db2-eddebaa71383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_scaler= Pipeline([('scaler', StandardScaler()), ('model', DecisionTreeRegressor(max_depth=best_depth, random_state=42))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486939c5-d530-4b03-8ded-6e3981b0a8db",
   "metadata": {},
   "source": [
    "we fit both models with and without scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870b352-558b-4813-bfae-93da802bbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_no_scaler.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21240084-6e71-4d2d-ae48-13e72d2efc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_scaler.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba38e24-caa9-43b9-a3de-2b2245612cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_scaler = pipe_no_scaler.predict(X_test)\n",
    "y_pred_scaler = pipe_scaler.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6d206-171f-4fa7-a6bf-3dc19783d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_no_scaler = mean_squared_error(y_test, y_pred_no_scaler)\n",
    "rmse_scaler = mean_squared_error(y_test, y_pred_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f6d94-e6cf-431b-824f-a517b4055e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE without scaling:\", rmse_no_scaler)\n",
    "print(\"RMSE with scaling:\", rmse_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b65b1-bb49-4ba1-bc90-9bb208d93e19",
   "metadata": {},
   "source": [
    "We can see that using the StandartScaler does not change the performance of the  DecisionTreeRegressor here, the RMSE is exactly the same with or without scaling. We can explain that with the fact that all numerical values have approximately the same scale and are not in a wide range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76e20f-e701-44d1-a23f-09b1a7c4bbe8",
   "metadata": {},
   "source": [
    "8) For a LinearRegression model with fit_intercept=True, test the impact of using a\n",
    "StandardScaler. Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4a1ee-67a6-45e9-8894-961fb6b34722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fa24f-4591-4944-a518-d2cd8192c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_no_scaler = Pipeline([\n",
    "    (\"model\", LinearRegression(fit_intercept=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3d52d-c859-4d2d-a1c5-0e97a84deea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_scaler = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression(fit_intercept=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09161b82-7c2b-48ac-b6b9-2f1b54008fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_no_scaler.fit(X_train, y_train)\n",
    "pipe_scaler.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb8767-108e-4453-9cf8-97ea9aee6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_scaler = pipe_no_scaler.predict(X_test)\n",
    "y_pred_scaler = pipe_scaler.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbbfbd-9ff9-4c1f-abd7-1a0df1f27e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_no_scaler = mean_squared_error(y_test, y_pred_no_scaler)\n",
    "rmse_scaler = mean_squared_error(y_test, y_pred_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f371e4f-9e04-4a1e-a801-c005354fe933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE without scaling:\", rmse_no_scaler)\n",
    "print(\"RMSE with scaling:\", rmse_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73423511-859d-4045-b4f1-be058eeb9de3",
   "metadata": {},
   "source": [
    "We see again no difference in the RMSE with or without StandartScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0601230-dd53-47ee-9869-6185e4e977e1",
   "metadata": {},
   "source": [
    "We see again no difference with or without StandartScaler for the LinearRegression model, and that confirms our hypothesis on the range of the numerical datas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c562017-28f9-4120-8860-8a87e0603f07",
   "metadata": {},
   "source": [
    "9) Create a one hot encoder instance, fit it on the data, transform the data and display all categories\n",
    "inferred by the transformer. Delete the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adf85d-5629-48e8-990e-556f2e6042dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"Categorical columns:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acea1c-c4b3-44ef-9d1d-e60d56003bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "ohe.fit(df[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebc900-1093-49a2-8cba-b88ac5925a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, cats in zip(cat_cols, ohe.categories_):\n",
    "    print(f\" {col}: {len(cats)} categories\")\n",
    "    print(cats[:10], \"...\" if len(cats) > 10 else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3b31c-9007-4261-9622-74d654fcd830",
   "metadata": {},
   "source": [
    "10) Create a Pipeline standardizing the numerical features, and one-hot encoding categorical features,\n",
    "followed by the application of a RandomForestRegressor to the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d77891-edf3-44f3-beda-d0c8a6592107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc50223-aeba-46b4-bfde-d0dfa09cc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor= ColumnTransformer(transformers= [\n",
    "    ('num',StandardScaler(),num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e74a5-c532-46d8-88bb-9fe9d491621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinefinal= Pipeline(steps= [\n",
    "    ('preprocess',preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e7d4c-4325-4b52-bc9d-b458735e3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2b26f-0881-4ab5-8c30-4421ba5534cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinefinal.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146968a-4229-47eb-a4c9-f36104e36008",
   "metadata": {},
   "source": [
    "We have our pipeline with the numerical features standardized by StandardScaler, our categorical features encoded with OneHotEncoder and we applied the RandomForestRegressor to the datas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bb049-fdf5-42e3-b91d-0e88ccd4d9d1",
   "metadata": {},
   "source": [
    "11) Perform grid-search on the cross-validation error to tune simultaneously the n_estimators and\n",
    "max_depth of the prediction step of your pipeline. Comment on the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f7dd9-64b4-4d9d-bd8e-e7694a0e7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridscv= GridSearchCV(\n",
    "    estimator=pipelinefinal,\n",
    "    param_grid= {'model__n_estimators': [50, 100, 200],\n",
    "                 'model__max_depth': [None, 10,20,30]},\n",
    "    cv= 5,\n",
    "    scoring= 'neg_root_mean_squared_error',\n",
    "    n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73e67a-3160-4efb-bdc5-de03e8f919d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start= time.time()\n",
    "gridscv.fit(X, y)\n",
    "end=time.time()\n",
    "print(' execution time : {} seconds'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68454b-4c89-4203-9a8d-39a061175c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5863f36-e66a-4ff9-a529-1863b8e151c5",
   "metadata": {},
   "source": [
    "The execution of grisearchcv.fit took almost 15 seconds, it is due to the fact that we put cv= 5, because the model was trained 5 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e681ed-1ba0-44d5-94d3-a5e3634b309a",
   "metadata": {},
   "source": [
    "12) Get the estimator with the best params. Save both the full pipeline and the best model to disk\n",
    "with joblib. Load them from disk. Why is the ability to dump estimators useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6266e-025d-44b3-b0f9-fdf9cb315d23",
   "metadata": {},
   "source": [
    "We get the estimator with the best parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd257e-7153-4a12-a298-309370260d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = gridscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a14f8-6428-405e-8270-f5f5c8981eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4f170-77c0-4b87-845c-6b256d0fa162",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best, 'pipeline_lab3.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df9348-7369-4cca-904e-64ecef31300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_loaded= joblib.load('pipeline_lab3.joblib')\n",
    "pipe_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb581ac-fc38-419b-8581-e8632422963f",
   "metadata": {},
   "source": [
    "It is very useful to save pipelines and use them again in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74333b94-7aaa-485d-834b-f9676cf23dc4",
   "metadata": {},
   "source": [
    "13) What is the cost of fitting a KNN? and of predicting for one new point?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8233f6-ea10-472d-869b-fefc3be7347e",
   "metadata": {},
   "source": [
    "The cost of fitting a KNN is the cost of storing all the datas, and the cost of predicting for one new point is the cost of computing the distance beteween the new point and the k points associated with the training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb106199-f854-4627-be84-5e2a0ed98d4f",
   "metadata": {},
   "source": [
    "14) Implement a KNearestNeighbor class with init , fit and predict. scipy.stats.mode may\n",
    "be useful for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b74239-016a-467c-b0ae-28fe21baa927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6859c-f858-4f30-8848-8ca7eb48eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor:\n",
    "    def __init__(self, n_neighbors= 5):\n",
    "        self.k= n_neighbors\n",
    "    def fit(self, X,y):\n",
    "        self.X_train= np.array(X)\n",
    "        self.y_train= np.array(y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        X=np.array(X)\n",
    "        y_pred= []\n",
    "        for x in X:\n",
    "            distances= np.sqrt(((self.X_train-x)**2).sum(axis=1))\n",
    "            idx= np.argsort(distances)[:self.k]\n",
    "            vote=mode(self.y_train[idx], keepdims= False).mode\n",
    "            y_pred.append(vote)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f959f0c-fcb6-48a0-a13e-c6496ffe4893",
   "metadata": {},
   "source": [
    "15) Generate data with the function rand checkers on Moodle. Describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a2e2f-3954-4f9b-bf5e-2eb99ab0159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "def rand_checkers(n_samples=200, sigma=0.1, random_state=0):\n",
    "    rng = check_random_state(random_state)\n",
    "    nbp = n_samples // 16\n",
    "    nbn = n_samples // 16\n",
    "    xapp = rng.rand((nbp + nbn) * 16).reshape((nbp + nbn) * 8, 2)\n",
    "    yapp = np.ones((nbp + nbn) * 8)\n",
    "    idx = 0\n",
    "    for i in range(-2, 2):\n",
    "        for j in range(-2, 2):\n",
    "            if ((i + j) % 2) == 0:\n",
    "                nb = nbp\n",
    "            else:\n",
    "                nb = nbn\n",
    "                yapp[idx:(idx + nb)] = [(i + j) % 3 + 1] * nb\n",
    "\n",
    "            xapp[idx:(idx + nb), 0] = rng.rand(nb)\n",
    "            xapp[idx:(idx + nb), 0] += i + sigma * rng.randn(nb)\n",
    "            xapp[idx:(idx + nb), 1] = rng.rand(nb)\n",
    "            xapp[idx:(idx + nb), 1] += j + sigma * rng.randn(nb)\n",
    "            idx += nb\n",
    "\n",
    "    ind = np.arange(xapp.shape[0])\n",
    "    rng.shuffle(ind)\n",
    "    res = np.hstack([xapp, yapp[:, np.newaxis]])\n",
    "    return np.array(res[ind, :2]), np.array(res[ind, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352b2a0-d428-47d1-a20e-db06db94aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = rand_checkers(n_samples=300)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89dbd4-3d95-4291-aa50-a711a18dd7c8",
   "metadata": {},
   "source": [
    "This data show three different classes of points , with 3 different colours: blue, red and white."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28737cd-d08d-48e2-a7c9-732279ba4db6",
   "metadata": {},
   "source": [
    "16) Use 10 fold cross validation to tune the parameter K of your estimator on this dataset (it may\n",
    "help to have your having your class inherit from BaseEstimator and ClassifierMixin, that can\n",
    "be imported from sklearn.base). Plot the average scores on the train and test sets as a function\n",
    "of K. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa2497-d901-4fc2-b92b-003419b3b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b0d19-1ff4-4d2d-98c3-a7f2ea365a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = rand_checkers(n_samples=300)\n",
    "K_values = range(1, 21)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for K in K_values:\n",
    "    knn = KNearestNeighbor(n_neighbors=K)\n",
    "    knn.fit(X, y)\n",
    "    y_pred_train = knn.predict(X)\n",
    "    train_acc = np.mean(y_pred_train == y)\n",
    "    train_scores.append(train_acc)\n",
    "    accs = []\n",
    "    for train_idx, test_idx in cv.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        knn = KNearestNeighbor(n_neighbors=K)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accs.append(np.mean(y_pred == y_test))\n",
    "    test_scores.append(np.mean(accs))\n",
    "\n",
    "plt.plot(K_values, train_scores, 'o-', label='Train accuracy')\n",
    "plt.plot(K_values, test_scores, 's-', label='Cross-validation accuracy')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('10-fold CV for KNN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676e4ed-ff41-43c3-ab1b-398526c3f4f9",
   "metadata": {},
   "source": [
    "We can see that as K is increasing, the model gets smoother and the training accuracy decreases, whereas the cross validation accuracy improves a little before going down again. The best K seems to be around 10, it is where the model performs the most consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4633ce",
   "metadata": {},
   "source": [
    "## - Encoding and hyperparameter tuning with Optuna -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1dbd08",
   "metadata": {},
   "source": [
    "17) On the Adult census dataset used in class, compare the performances of LogisticRegression, RandomForest and HistGradientBoostingClassifier (with the default hyperparameter values) on one hot vs ordinal encoded data. How does the chosen encoding affects each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83135096",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_census = pd.read_csv(\"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253eb07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  encoding                 model  mean_accuracy       std\n",
      "0  one-hot  HistGradientBoosting       0.873551  0.002877\n",
      "1  ordinal  HistGradientBoosting       0.873408  0.002487\n",
      "2  one-hot    LogisticRegression       0.852197  0.003184\n",
      "3  ordinal    LogisticRegression       0.824966  0.002354\n",
      "4  one-hot          RandomForest       0.854531  0.003381\n",
      "5  ordinal          RandomForest       0.857582  0.002738\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoding</th>\n",
       "      <th>model</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one-hot</td>\n",
       "      <td>HistGradientBoosting</td>\n",
       "      <td>0.873551</td>\n",
       "      <td>0.002877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ordinal</td>\n",
       "      <td>HistGradientBoosting</td>\n",
       "      <td>0.873408</td>\n",
       "      <td>0.002487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one-hot</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.852197</td>\n",
       "      <td>0.003184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ordinal</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.824966</td>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one-hot</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.854531</td>\n",
       "      <td>0.003381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ordinal</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.857582</td>\n",
       "      <td>0.002738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  encoding                 model  mean_accuracy       std\n",
       "0  one-hot  HistGradientBoosting       0.873551  0.002877\n",
       "1  ordinal  HistGradientBoosting       0.873408  0.002487\n",
       "2  one-hot    LogisticRegression       0.852197  0.003184\n",
       "3  ordinal    LogisticRegression       0.824966  0.002354\n",
       "4  one-hot          RandomForest       0.854531  0.003381\n",
       "5  ordinal          RandomForest       0.857582  0.002738"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "df_adult = adult_census.replace({\"?\": np.nan}).copy()\n",
    "\n",
    "y_adult = df_adult[\"class\"].astype(str).str.contains(\">\", regex=False).astype(int)\n",
    "X_adult = df_adult.drop(columns=[\"class\"])  # features\n",
    "\n",
    "num_cols = X_adult.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_adult.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Preprocessing\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_ohe_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "cat_ord_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "])\n",
    "\n",
    "preprocess_ohe = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_ohe_pipe, cat_cols),\n",
    "])\n",
    "\n",
    "preprocess_ord = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_ord_pipe, cat_cols),\n",
    "])\n",
    "\n",
    "# Models (defaults)\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rows = []\n",
    "for enc_name, pre in [(\"one-hot\", preprocess_ohe), (\"ordinal\", preprocess_ord)]:\n",
    "    for model_name, model in models.items():\n",
    "        pipe = Pipeline([\n",
    "            (\"preprocess\", pre),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        scores = cross_val_score(pipe, X_adult, y_adult, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "        rows.append({\n",
    "            \"encoding\": enc_name,\n",
    "            \"model\": model_name,\n",
    "            \"mean_accuracy\": scores.mean(),\n",
    "            \"std\": scores.std(),\n",
    "        })\n",
    "\n",
    "res_df = pd.DataFrame(rows).sort_values([\"model\", \"encoding\"]).reset_index(drop=True)\n",
    "print(res_df)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610942b7",
   "metadata": {},
   "source": [
    "- LogisticRegression: one-hot gives the best accuracy. Linear models need a separate weight per category. Ordinal codes create a fake order that hurts.\n",
    "- RandomForest: both encodings work. One-hot helps isolate categories; ordinal also works. The difference is small in practice.\n",
    "- HistGradientBoostingClassifier: ordinal tends to do better than one-hot. This model prefers compact dense inputs instead of very wide one-hot features.\n",
    "\n",
    "BREF: encoding choice depends on the model. Use one-hot for linear models; prefer ordinal (or native categorical) for histogram-based boosting; forests are robust to both. Always impute missing values and standardize numeric features first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820066c",
   "metadata": {},
   "source": [
    "18) Use optuna to tune the learning rate of a HistogramGradienBoostClassifier. Compare to the performance of the other two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ffb7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 17:49:08,744] A new study created in memory with name: no-name-706e0397-8db7-4a6f-86a9-9701ea7d4b2f\n",
      "[I 2025-11-13 17:49:09,227] Trial 0 finished with value: 0.870828364960313 and parameters: {'learning_rate': 0.3308561750339167}. Best is trial 0 with value: 0.870828364960313.\n",
      "[I 2025-11-13 17:49:10,227] Trial 1 finished with value: 0.8686785947022407 and parameters: {'learning_rate': 0.029610852394074672}. Best is trial 0 with value: 0.870828364960313.\n",
      "[I 2025-11-13 17:49:10,934] Trial 2 finished with value: 0.872650606151988 and parameters: {'learning_rate': 0.2457513906346491}. Best is trial 2 with value: 0.872650606151988.\n",
      "[I 2025-11-13 17:49:11,832] Trial 3 finished with value: 0.8680439067940349 and parameters: {'learning_rate': 0.027837316063434984}. Best is trial 2 with value: 0.872650606151988.\n",
      "[I 2025-11-13 17:49:12,290] Trial 4 finished with value: 0.8711765283369337 and parameters: {'learning_rate': 0.40643960076053387}. Best is trial 2 with value: 0.872650606151988.\n",
      "[I 2025-11-13 17:49:13,252] Trial 5 finished with value: 0.8718520845452977 and parameters: {'learning_rate': 0.04714942044950728}. Best is trial 2 with value: 0.872650606151988.\n",
      "[I 2025-11-13 17:49:14,470] Trial 6 finished with value: 0.8676548960764336 and parameters: {'learning_rate': 0.025448086539942624}. Best is trial 2 with value: 0.872650606151988.\n",
      "[I 2025-11-13 17:49:15,783] Trial 7 finished with value: 0.873305771176587 and parameters: {'learning_rate': 0.06441869480953528}. Best is trial 7 with value: 0.873305771176587.\n",
      "[I 2025-11-13 17:49:17,153] Trial 8 finished with value: 0.8697022912321305 and parameters: {'learning_rate': 0.033793919800933166}. Best is trial 7 with value: 0.873305771176587.\n",
      "[I 2025-11-13 17:49:18,378] Trial 9 finished with value: 0.8729781750408222 and parameters: {'learning_rate': 0.08588777024512688}. Best is trial 7 with value: 0.873305771176587.\n",
      "[I 2025-11-13 17:49:19,723] Trial 10 finished with value: 0.853957654413177 and parameters: {'learning_rate': 0.010230299849529355}. Best is trial 7 with value: 0.873305771176587.\n",
      "[I 2025-11-13 17:49:20,918] Trial 11 finished with value: 0.873612890195959 and parameters: {'learning_rate': 0.12227233102457379}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:22,095] Trial 12 finished with value: 0.8726301143641711 and parameters: {'learning_rate': 0.12807318100276174}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:23,263] Trial 13 finished with value: 0.8723025161324884 and parameters: {'learning_rate': 0.14956832588289332}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:24,624] Trial 14 finished with value: 0.8732033939782943 and parameters: {'learning_rate': 0.0710554187007008}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:25,772] Trial 15 finished with value: 0.8732443188682313 and parameters: {'learning_rate': 0.15133456952380817}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:26,939] Trial 16 finished with value: 0.8734286108198484 and parameters: {'learning_rate': 0.09376438767907495}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:27,854] Trial 17 finished with value: 0.8722410931669812 and parameters: {'learning_rate': 0.21283459793517376}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:29,073] Trial 18 finished with value: 0.8733467002583595 and parameters: {'learning_rate': 0.09331034736829324}. Best is trial 11 with value: 0.873612890195959.\n",
      "[I 2025-11-13 17:49:30,430] Trial 19 finished with value: 0.8721796639137207 and parameters: {'learning_rate': 0.048578582516455776}. Best is trial 11 with value: 0.873612890195959.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_lr': 0.12227233102457379, 'best_cv_acc': 0.873612890195959}\n",
      "For comparison res_df =   encoding                 model  mean_accuracy       std\n",
      "0  one-hot  HistGradientBoosting       0.873551  0.002877\n",
      "1  ordinal  HistGradientBoosting       0.873408  0.002487\n",
      "2  one-hot    LogisticRegression       0.852197  0.003184\n",
      "3  ordinal    LogisticRegression       0.824966  0.002354\n",
      "4  one-hot          RandomForest       0.854531  0.003381\n",
      "5  ordinal          RandomForest       0.857582  0.002738\n",
      "                                            model  mean_accuracy\n",
      "0  HistGradientBoosting tuned lr=0.1223 (ordinal)       0.873613\n",
      "1          HistGradientBoosting default (ordinal)       0.873408\n",
      "2                          RandomForest (ordinal)       0.857582\n",
      "3                    LogisticRegression (one-hot)       0.852197\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "df_adult = adult_census.replace({\"?\": np.nan}).copy()\n",
    "\n",
    "\n",
    "# target: 1 if >50K else 0; column may be named 'class' or 'income'\n",
    "possible_targets = [\"class\", \"income\"]\n",
    "target_col = next(col for col in possible_targets if col in df_adult.columns)\n",
    "X = df_adult.drop(columns=[target_col])\n",
    "y = (df_adult[target_col].astype(str).str.contains(\">\", regex=False)).astype(int)\n",
    "\n",
    "# Column splits\n",
    "num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Preprocessing blocks\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_ohe_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "cat_ord_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "])\n",
    "\n",
    "preprocess_ohe = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_ohe_pipe, cat_cols),\n",
    "])\n",
    "\n",
    "preprocess_ord = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_ord_pipe, cat_cols),\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Optuna objective: maximize accuracy via 5-fold CV on ordinal-encoded features\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-2, 5e-1, log=True)\n",
    "    model = HistGradientBoostingClassifier(learning_rate=lr, random_state=42)\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocess_ord),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "    scores = cross_val_score(pipe, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "best_lr = study.best_params[\"learning_rate\"]\n",
    "best_score = study.best_value\n",
    "\n",
    "# Compare tuned HGBT to defaults of the other two models (encodings chosen per Q17 insights)\n",
    "rows = []\n",
    "\n",
    "# LogisticRegression with one-hot\n",
    "pipe_lr = Pipeline([\n",
    "    (\"preprocess\", preprocess_ohe),\n",
    "    (\"model\", LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "rows.append({\n",
    "    \"model\": \"LogisticRegression (one-hot)\",\n",
    "    \"mean_accuracy\": cross_val_score(pipe_lr, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean(),\n",
    "})\n",
    "\n",
    "# RandomForest with ordinal (robust either way)\n",
    "pipe_rf = Pipeline([\n",
    "    (\"preprocess\", preprocess_ord),\n",
    "    (\"model\", RandomForestClassifier(random_state=42)),\n",
    "])\n",
    "rows.append({\n",
    "    \"model\": \"RandomForest (ordinal)\",\n",
    "    \"mean_accuracy\": cross_val_score(pipe_rf, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean(),\n",
    "})\n",
    "\n",
    "# HGBT default (ordinal)\n",
    "pipe_hgb_default = Pipeline([\n",
    "    (\"preprocess\", preprocess_ord),\n",
    "    (\"model\", HistGradientBoostingClassifier(random_state=42)),\n",
    "])\n",
    "rows.append({\n",
    "    \"model\": \"HistGradientBoosting default (ordinal)\",\n",
    "    \"mean_accuracy\": cross_val_score(pipe_hgb_default, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean(),\n",
    "})\n",
    "\n",
    "# HGBT tuned (ordinal)\n",
    "pipe_hgb_tuned = Pipeline([\n",
    "    (\"preprocess\", preprocess_ord),\n",
    "    (\"model\", HistGradientBoostingClassifier(learning_rate=best_lr, random_state=42)),\n",
    "])\n",
    "rows.append({\n",
    "    \"model\": f\"HistGradientBoosting tuned lr={best_lr:.4f} (ordinal)\",\n",
    "    \"mean_accuracy\": cross_val_score(pipe_hgb_tuned, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean(),\n",
    "})\n",
    "\n",
    "res18_df = pd.DataFrame(rows).sort_values(\"mean_accuracy\", ascending=False).reset_index(drop=True)\n",
    "print({\"best_lr\": best_lr, \"best_cv_acc\": best_score})\n",
    "print(res18_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0112b27",
   "metadata": {},
   "source": [
    "- Tuning only the learning_rate with Optuna yields a small but consistent accuracy gain over the default HistGradientBoostingClassifier.\n",
    "- The tuned HGBT is typically on par with or better than RandomForest, and clearly stronger than LogisticRegression on this dataset when using ordinal encoding for categorical variables.\n",
    "- Why: histogram-based boosting benefits from a well-chosen learning rate and compact ordinal features; one-hot would make inputs very wide and is not necessary here.\n",
    "\n",
    "BREF: a quick Optuna search on learning_rate improves HGBT over its default and makes it competitive/best among the three models on Adult Census."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415c5a5",
   "metadata": {},
   "source": [
    "## - Processing fuzzy categorical data -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293b314",
   "metadata": {},
   "source": [
    "19) Load the salary_X and salary_y data from the csv files on moodle (beware of index columns). What’s this dataset about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6133d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  gender department                              department_name  \\\n",
       " 0      F        POL                         Department of Police   \n",
       " 1      M        POL                         Department of Police   \n",
       " 2      F        HHS      Department of Health and Human Services   \n",
       " 3      M        COR                Correction and Rehabilitation   \n",
       " 4      M        HCA  Department of Housing and Community Affairs   \n",
       " \n",
       "                                             division assignment_category  \\\n",
       " 0  MSB Information Mgmt and Tech Division Records...    Fulltime-Regular   \n",
       " 1         ISB Major Crimes Division Fugitive Section    Fulltime-Regular   \n",
       " 2      Adult Protective and Case Management Services    Fulltime-Regular   \n",
       " 3                         PRRS Facility and Security    Fulltime-Regular   \n",
       " 4                        Affordable Housing Programs    Fulltime-Regular   \n",
       " \n",
       "        employee_position_title date_first_hired  year_first_hired  \n",
       " 0  Office Services Coordinator       09/22/1986              1986  \n",
       " 1        Master Police Officer       09/12/1988              1988  \n",
       " 2             Social Worker IV       11/19/1989              1989  \n",
       " 3       Resident Supervisor II       05/05/2014              2014  \n",
       " 4      Planning Specialist III       03/05/2007              2007  ,\n",
       " 0     69222.18\n",
       " 1     97392.47\n",
       " 2    104717.28\n",
       " 3     52734.57\n",
       " 4     93396.00\n",
       " Name: current_annual_salary, dtype: float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  salary_X = pd.read_csv('salary_X.csv', index_col=0)\n",
    "  salary_y = pd.read_csv('salary_y.csv', index_col=0).squeeze(\"columns\")\n",
    "\n",
    "# In case the files are not found, read from the ZIP archive\n",
    "except FileNotFoundError:\n",
    "  from zipfile import ZipFile\n",
    "\n",
    "  # Read the specific CSVs within the ZIP and drop the index column if present\n",
    "  with ZipFile('skrub_data_lab3.zip') as z:\n",
    "      with z.open('salary_X.csv') as f:\n",
    "          salary_X = pd.read_csv(f, index_col=0)\n",
    "      with z.open('salary_y.csv') as f:\n",
    "          salary_y = pd.read_csv(f, index_col=0).squeeze(\"columns\")  # Series if single column\n",
    "\n",
    "from typing import cast\n",
    "salary_X = cast(pd.DataFrame, salary_X)\n",
    "salary_y = cast(pd.Series, salary_y)\n",
    "\n",
    "salary_X.head(), salary_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c27cfd",
   "metadata": {},
   "source": [
    "The dataset is about employees caracteristics (gender, department, department_name, division, assignment_category, employee_position_title, date_first_hired, year_first_hired) and their corresponding salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641d9a6",
   "metadata": {},
   "source": [
    "20) How many distinct modalities are there per column of X? When using a One Hot Encoder, which columns may cause issues ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da19cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical column cardinalities (sorted):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_unique</th>\n",
       "      <th>pct_unique</th>\n",
       "      <th>high_cardinality_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date_first_hired</th>\n",
       "      <td>2264</td>\n",
       "      <td>24.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>division</th>\n",
       "      <td>694</td>\n",
       "      <td>7.52</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employee_position_title</th>\n",
       "      <td>443</td>\n",
       "      <td>4.80</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>department</th>\n",
       "      <td>37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>department_name</th>\n",
       "      <td>37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignment_category</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         n_unique  pct_unique  high_cardinality_risk\n",
       "date_first_hired             2264       24.53                   True\n",
       "division                      694        7.52                   True\n",
       "employee_position_title       443        4.80                   True\n",
       "department                     37        0.40                  False\n",
       "department_name                37        0.40                  False\n",
       "gender                          2        0.02                  False\n",
       "assignment_category             2        0.02                  False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify candidate categorical columns (object or category dtypes)\n",
    "cat_cols_salary = [c for c in salary_X.columns if salary_X[c].dtype == 'object' or str(salary_X[c].dtype).startswith('category')]\n",
    "cardinality = (\n",
    "    salary_X[cat_cols_salary]\n",
    "    .nunique(dropna=True)\n",
    "    .sort_values(ascending=False)\n",
    "    .rename('n_unique')\n",
    "    .to_frame()\n",
    ")\n",
    "cardinality['pct_unique'] = (cardinality['n_unique'] / len(salary_X) * 100).round(2)\n",
    "\n",
    "# Flag high-cardinality risk (arbitrary threshold > 50 unique values and > 1% distinct ratio)\n",
    "cardinality['high_cardinality_risk'] = (cardinality['n_unique'] > 50) & (cardinality['pct_unique'] > 1.0)\n",
    "\n",
    "print(\"Categorical column cardinalities (sorted):\")\n",
    "cardinality.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e5841",
   "metadata": {},
   "source": [
    "- The table above lists unique counts per categorical column with the percentage of unique values.\n",
    "- Columns with a high number of distinct values (here > 50) may cause issue creating huge One Hot Encoding Matrix, such as date_first_hire, division or employee_position_title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280985b0",
   "metadata": {},
   "source": [
    "21) Inspect the column employee_position_title. Is there a natural notion of distance on those modalities? Are the modalities completely unordered? Which encoding would you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f52e166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles: 443\n",
      "Top 15 titles:\n",
      "employee_position_title\n",
      "Bus Operator                               638\n",
      "Police Officer III                         620\n",
      "Firefighter/Rescuer III                    388\n",
      "Manager III                                243\n",
      "Firefighter/Rescuer II                     219\n",
      "Master Firefighter/Rescuer                 218\n",
      "Office Services Coordinator                207\n",
      "School Health Room Technician I            204\n",
      "Police Officer II                          171\n",
      "Community Health Nurse II                  165\n",
      "Crossing Guard                             161\n",
      "Correctional Officer III (Corporal)        151\n",
      "Program Manager II                         145\n",
      "Income Assistance Program Specialist II    142\n",
      "Fire/Rescue Captain                        141\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Examples of near-duplicate titles (normalized, similarity >= 0.9):\n",
      "- 'public safety emergency communications specialist iii'  ~  'public safety emergency communications specialist ii'  (sim=0.99)\n",
      "- 'deputy sheriff iii'  ~  'deputy sheriff ii'  (sim=0.971)\n",
      "- 'deputy sheriff iii'  ~  'deputy sheriff i'  (sim=0.941)\n",
      "- 'work force leader iii'  ~  'work force leader ii'  (sim=0.976)\n",
      "- 'work force leader iii'  ~  'work force leader i'  (sim=0.95)\n",
      "- 'work force leader iii'  ~  'work force leader iv'  (sim=0.927)\n",
      "- 'legislative analyst ii'  ~  'legislative analyst iii'  (sim=0.978)\n",
      "- 'legislative analyst ii'  ~  'legislative analyst i'  (sim=0.977)\n",
      "- 'imaging operator ii'  ~  'imaging operator i'  (sim=0.973)\n",
      "- 'medical doctor iv psychiatrist'  ~  'medical doctor iii psychiatrist'  (sim=0.951)\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher, get_close_matches\n",
    "\n",
    "col = 'employee_position_title'\n",
    "assert col in salary_X.columns, f\"Column '{col}' not found in salary_X\"\n",
    "\n",
    "# Basic stats\n",
    "n_unique = salary_X[col].nunique(dropna=True)\n",
    "print(f\"Unique titles: {n_unique}\")\n",
    "print(\"Top 15 titles:\")\n",
    "print(salary_X[col].value_counts(dropna=True).head(15))\n",
    "\n",
    "# Normalize text to expose near duplicates\n",
    "norm = (\n",
    "    salary_X[col]\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9 ]+\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "unique_norm = norm.dropna().unique().tolist()\n",
    "\n",
    "# Sample a subset to search for close matches\n",
    "rng = np.random.default_rng(42)\n",
    "sample_titles = rng.choice(unique_norm, size=min(50, len(unique_norm)), replace=False)\n",
    "\n",
    "examples = []\n",
    "for t in sample_titles:\n",
    "    matches = get_close_matches(t, unique_norm, n=5, cutoff=0.9)\n",
    "    for m in matches:\n",
    "        if m != t:\n",
    "            ratio = SequenceMatcher(None, t, m).ratio()\n",
    "            if ratio >= 0.9:\n",
    "                examples.append((t, m, round(ratio, 3)))\n",
    "\n",
    "# Deduplicate symmetric pairs\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "for a, b, r in examples:\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append((a, b, r))\n",
    "\n",
    "print(\"\\nExamples of near-duplicate titles (normalized, similarity >= 0.9):\")\n",
    "for a, b, r in unique_pairs[:10]:\n",
    "    print(f\"- '{a}'  ~  '{b}'  (sim={r})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8d447",
   "metadata": {},
   "source": [
    "- A natural distance would be easily implemented using alphabetical distance (not sure of the name) after regex transformation and dropping of uninteresting characters.\n",
    "- The modalities are somewhat ordered in terms of seniority or groups inside of groups, thus they encode further information (intuitively, Manager III earns more then Manager I).\n",
    "- One Hot Encoding would explode the dimensionality and avoid the sharing of information, one better encoding would be the *GapEncoder* (strangely the next question)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ccef9",
   "metadata": {},
   "source": [
    "To tackle this problem of fuzzy labelling, we will use the GapEncoder from the skrub package (documentation https://skrub-data.org/stable/). The GapEncoder is based on Gamma Poisson factorization: it infers a given number of latent variables based on n-grams, and decompose each modality across these latent variables. Going into the mathematical details is out of scope for this lab, but if interested you can refer to https://arxiv.org/abs/1907.01860."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a6c7c",
   "metadata": {},
   "source": [
    "22) Detail the output of a GapEncoder when used on the following data:\n",
    "\n",
    "```\n",
    "X = [[\"Math, optimization\"], [\"mathematics\"], [\"maths, ml\"], [\"ml.maths\"],[\"machine learning\"], [\"physics\"], [\"phy\"], [\"statistical physics\"], [\"computational phys.\"]]\n",
    "```\n",
    "\n",
    "Compare the output of the trained encoder on clean and dirty modalities, eg [\"physics\"] vs [\"physcis\"]. Is the behavior you observe a good thing or a bad thing? What does the n_components represent? Print the learned components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7a44dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Transformed Output for X (X_transformed.shape = (9, 2)) ###\n",
      "   learning, machine, mathematics  computational, statistical, physics\n",
      "0                        1.495992                             0.075747\n",
      "1                        0.880239                             0.006717\n",
      "2                        0.687950                             0.003354\n",
      "3                        0.590136                             0.003342\n",
      "4                        1.372740                             0.003347\n",
      "5                        0.003871                             0.491781\n",
      "6                        0.003280                             0.101067\n",
      "7                        0.004084                             1.665481\n",
      "8                        0.003541                             1.666024\n",
      "----------------------------------------\n",
      "Output for 'physics':   clean_transformed =    learning, machine, mathematics  computational, statistical, physics\n",
      "0                        0.003871                             0.491781\n",
      "Output for 'physcis':  dirty_transformed =    learning, machine, mathematics  computational, statistical, physics\n",
      "0                         0.00333                             0.231453\n"
     ]
    }
   ],
   "source": [
    "from skrub import GapEncoder\n",
    "\n",
    "X_list = [\"Math, optimization\", \"mathematics\", \"maths, ml\", \"ml.maths\",\n",
    "          \"machine learning\", \"physics\", \"phy\", \"statistical physics\", \n",
    "          \"computational phys.\"]\n",
    "\n",
    "X = pd.Series(X_list)\n",
    "\n",
    "encoder = GapEncoder(n_components=2, random_state=42)\n",
    "X_transformed = encoder.fit_transform(X) # type: ignore\n",
    "\n",
    "print(f\"### Transformed Output for X ({X_transformed.shape = }) ###\")\n",
    "print(X_transformed)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "clean_data = pd.Series([\"physics\"])\n",
    "dirty_data = pd.Series([\"physcis\"])\n",
    "\n",
    "clean_transformed = encoder.transform(clean_data)\n",
    "dirty_transformed = encoder.transform(dirty_data)\n",
    "\n",
    "print(f\"Output for 'physics':   {clean_transformed = }\")\n",
    "print(f\"Output for 'physcis':  {dirty_transformed = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163ca80",
   "metadata": {},
   "source": [
    "- GapEncoder maps each string to a dense vector of length n_components (here 2); the fitted matrix shows math/ML terms loading high on component‑0 and physics/computation on component‑1.\n",
    "- physics and physcis map nearby but not identically (the typo has reduced main-component magnitude) — useful for robustness to noisy labels.\n",
    "- n_components = number of latent topics; learned components ≈ \"learning/machine/mathematics\" and \"computational/statistical/physics\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfaa4a4",
   "metadata": {},
   "source": [
    "23) Create a pipeline with two steps: a TableVectorizer, and a HistGradientBoosting regressor. Fit it on the full X and y. Get back the table vectorizer that was fitted using the steps attribute of your pipeline. What did fit do here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4a4985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: mean=8853.63  std=645.01\n",
      "\n",
      "Vectorized feature names (first 30):\n",
      "[np.str_('gender_F'), np.str_('gender_M'), np.str_('gender_nan'), np.str_('department_BOA'), np.str_('department_BOE'), np.str_('department_CAT'), np.str_('department_CCL'), np.str_('department_CEC'), np.str_('department_CEX'), np.str_('department_COR'), np.str_('department_CUS'), np.str_('department_DEP'), np.str_('department_DGS'), np.str_('department_DHS'), np.str_('department_DLC'), np.str_('department_DOT'), np.str_('department_DPS'), np.str_('department_DTS'), np.str_('department_ECM'), np.str_('department_FIN'), np.str_('department_FRS'), np.str_('department_HCA'), np.str_('department_HHS'), np.str_('department_HRC'), np.str_('department_IGR'), np.str_('department_LIB'), np.str_('department_MPB'), np.str_('department_NDA'), np.str_('department_OAG'), np.str_('department_OCP')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "salary_X = pd.read_csv('salary_X.csv', index_col=0)\n",
    "salary_y = pd.read_csv('salary_y.csv', index_col=0).squeeze()\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "tv = TableVectorizer()\n",
    "model = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    (\"vectorize\", tv),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Use RMSE for readability\n",
    "rmse = (\n",
    "    -cross_val_score(pipe, salary_X, salary_y, cv=kf, scoring=\"neg_root_mean_squared_error\")  # type: ignore\n",
    ")\n",
    "print(f\"CV RMSE: mean={rmse.mean():.2f}  std={rmse.std():.2f}\")\n",
    "\n",
    "# Fit on full data to inspect learned feature mapping\n",
    "pipe.fit(salary_X, salary_y)  # type: ignore\n",
    "feat_names = pipe.named_steps[\"vectorize\"].get_feature_names_out()\n",
    "print(\"\\nVectorized feature names (first 30):\")\n",
    "print(list(feat_names[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160a1bc",
   "metadata": {},
   "source": [
    "Here, we `fit` on the vectorized X dataframe that is done \"automatically\" by `TableVectorizer` from the skrub library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d33622",
   "metadata": {},
   "source": [
    "# End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
